{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for doing keyword-focused sentiment analysis of raw\n",
    "text collected from news articles. \n",
    "\n",
    "Note: This notebook was originally developed in a DataBricks environment and will be adapted\n",
    "to run in AWS Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup and check mount to S3\n",
    "access_key = 'key goes here'\n",
    "secret_key = 'key goes here'\n",
    "encoded_secret_key = secret_key.replace('/', '%2F')\n",
    "bucket_name = 'bucket name goes here'\n",
    "mount_name = 's3_data'\n",
    "mount_resource = f\"s3a://{access_key}:{encoded_secret_key}@{bucket_name}\"\n",
    "mount_location = f\"/mnt/{mount_name}\"\n",
    "\n",
    "dbutils.fs.mount(mount_resource, mount_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(mount_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "yesterday = (datetime.today().date() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "path_to_yesterdays_articles = f\"{mount_location}/incoming/{yesterday}\"\n",
    "display(dbutils.fs.ls(path_to_yesterdays_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, lit, regexp_extract, length\n",
    "\n",
    "df = spark.read.text(path_to_yesterdays_articles) \\\n",
    "    .withColumn('filename', regexp_extract(input_file_name(), '[^/]+$', 0)) \\\n",
    "    .withColumn('date', lit(yesterday)) \\\n",
    "    .withColumnRenamed('value', 'article_text') \\\n",
    "    .withColumn('article_text_length', length('article_text')) \\\n",
    "    .filter('article_text_length > 400')\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.sql.functions import col, expr, array_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyTokenRemover(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(EmptyTokenRemover, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        remove_expr = array_remove(col(self.getInputCol()), '')\n",
    "        filtered_df = dataset.withColumn(self.getOutputCol(), remove_expr)\n",
    "        return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some basic cleaning and tokenization in preparation for getting term frequencies\n",
    "tokenizer = Tokenizer(inputCol='article_text', outputCol='tokens')\n",
    "empty_remover = EmptyTokenRemover(inputCol='tokens', outputCol='cleaner_tokens')\n",
    "stop_remover = StopWordsRemover(inputCol='cleaner_tokens', outputCol='stop_removed')\n",
    "\n",
    "prep_pipeline = Pipeline(stages=[tokenizer, empty_remover, stop_remover])\n",
    "\n",
    "prep_pipeline_model = prep_pipeline.fit(df)\n",
    "prepped_df = prep_pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, size, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Term Frequency for keyword (keyword e.g., 'pickleball')\n",
    "keyword = 'keyword goes here'\n",
    "\n",
    "tf_df = prepped_df.withColumn('keyword', lit(keyword)) \\\n",
    "    .withColumn('keyword_freq', expr(f\"size(filter(stop_removed, token -> token == '{keyword}'))\"))\n",
    "\n",
    "tf_df = tf_df.withColumn('norm_keyword_freq', col('keyword_freq') / size(col('stop_removed')))\n",
    "\n",
    "display(tf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(row[0], round(row[1], 4)) for row in (tf_df.select('filename', 'norm_keyword_freq').collect())], key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([ (row[0], round(row[1], 4)) for row in (tf_df.select('filename', 'norm_keyword_freq', 'keyword_freq').collect()) if row[2] > 2], key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword TF cutoffs (absolute count and relative to article word count) used here\n",
    "# determined somewhat arbitrarily through visual inspection of sample articles.\n",
    "\n",
    "keyword_freq_cutoff = 3\n",
    "norm_keyword_freq_cutoff = 0.003\n",
    "\n",
    "filtered_articles_df = tf_df.filter((col('keyword_freq') >= keyword_freq_cutoff) & (col('norm_keyword_freq') >= norm_keyword_freq_cutoff))\n",
    "\n",
    "display(filtered_articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "tokenize_sentences_udf = udf(tokenize_sentences, ArrayType(StringType()))\n",
    "\n",
    "sentence_tokenized_df = filtered_articles_df.withColumn('sentence', explode(tokenize_sentences_udf(filtered_articles_df['article_text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "sentence_score_udf = udf(sentiment_score, FloatType())\n",
    "\n",
    "sentence_sentiment_df = sentence_tokenized_df \\\n",
    "    .withColumn('sentiment_score', sentence_score_udf(sentence_tokenized_df['sentence'])) \\\n",
    "    .withColumn('keyword_in_sentence', col('sentence').contains(keyword))\n",
    "\n",
    "display(sentence_sentiment_df.select('filename', 'sentence', 'sentiment_score', 'keyword_in_sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_summary_df_1 = sentence_sentiment_df.groupBy('filename').agg(F.mean('sentiment_score').alias('full_article_sentiment'))\n",
    "\n",
    "sentiment_summary_df_2 = sentence_sentiment_df.filter(col('keyword_in_sentence') == True).groupBy('filename').agg(F.mean('sentiment_score').alias('keyword_sentences_sentiment'))\n",
    "\n",
    "sentiment_summary_df = sentiment_summary_df_1.join(sentiment_summary_df_2, 'filename', 'inner') \\\n",
    "    .withColumn('full_article_sentiment', F.format_number('full_article_sentiment', 3)) \\\n",
    "    .withColumn('keyword_sentences_sentiment', F.format_number('keyword_sentences_sentiment', 3))\n",
    "\n",
    "display(sentiment_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmount S3\n",
    "\n",
    "dbutils.fs.unmount(mount_location)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
