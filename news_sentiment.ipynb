{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for doing keyword-focused sentiment analysis of raw\n",
    "text collected from news articles. \n",
    "\n",
    "Note: This notebook was originally developed in a DataBricks environment and will be adapted\n",
    "to run in AWS Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850d3e49-e4d3-4007-bc12-4a1d98d134e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Setup and check mount to S3\n",
    "access_key = 'key goes here'\n",
    "secret_key = 'key goes here'\n",
    "encoded_secret_key = secret_key.replace('/', '%2F')\n",
    "bucket_name = 'bucket name goes here'\n",
    "mount_name = 's3_data'\n",
    "mount_resource = f\"s3a://{access_key}:{encoded_secret_key}@{bucket_name}\"\n",
    "mount_location = f\"/mnt/{mount_name}\"\n",
    "\n",
    "dbutils.fs.mount(mount_resource, mount_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35edc58-128f-4d56-a976-28476e558c41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(mount_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcf498f-0f2f-4783-9974-990b8cbc6adf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "yesterday = (datetime.today().date() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "path_to_yesterdays_articles = f\"{mount_location}/incoming/{yesterday}\"\n",
    "display(dbutils.fs.ls(path_to_yesterdays_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e84837-c61f-4534-9bd4-58c109c7f5d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14031655-9425-457e-bc81-e508e0a30800",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, lit, regexp_extract, length\n",
    "\n",
    "df = spark.read.text(path_to_yesterdays_articles) \\\n",
    "    .withColumn('filename', regexp_extract(input_file_name(), '[^/]+$', 0)) \\\n",
    "    .withColumn('date', lit(yesterday)) \\\n",
    "    .withColumnRenamed('value', 'article_text') \\\n",
    "    .withColumn('article_text_length', length('article_text')) \\\n",
    "    .filter('article_text_length > 400')\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af22551-4757-4397-b2ed-4945cac49882",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.sql.functions import col, expr, array_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc7b6b2-3b54-4d94-863d-ad69daef9a56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class EmptyTokenRemover(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(EmptyTokenRemover, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        remove_expr = array_remove(col(self.getInputCol()), '')\n",
    "        filtered_df = dataset.withColumn(self.getOutputCol(), remove_expr)\n",
    "        return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa183fc-045f-4493-a259-bc07fef4d7f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be498196-af73-4311-9cdc-158fe0053ccc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Do some basic cleaning and tokenization in preparation for getting term frequencies\n",
    "tokenizer = Tokenizer(inputCol='article_text', outputCol='tokens')\n",
    "empty_remover = EmptyTokenRemover(inputCol='tokens', outputCol='cleaner_tokens')\n",
    "stop_remover = StopWordsRemover(inputCol='cleaner_tokens', outputCol='stop_removed')\n",
    "\n",
    "prep_pipeline = Pipeline(stages=[tokenizer, empty_remover, stop_remover])\n",
    "\n",
    "prep_pipeline_model = prep_pipeline.fit(df)\n",
    "prepped_df = prep_pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3b069b-3e44-4837-8cba-a0119700d8b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, size, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec105a4f-1734-41d3-9825-6bb403fd282a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get Term Frequency for keyword (keyword e.g., 'pickleball')\n",
    "keyword = 'keyword goes here'\n",
    "\n",
    "tf_df = prepped_df.withColumn('keyword', lit(keyword)) \\\n",
    "    .withColumn('keyword_freq', expr(f\"size(filter(stop_removed, token -> token == '{keyword}'))\"))\n",
    "\n",
    "tf_df = tf_df.withColumn('norm_keyword_freq', col('keyword_freq') / size(col('stop_removed')))\n",
    "\n",
    "display(tf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b150bc-1464-45e0-976f-ee2f17e4e733",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted([(row[0], round(row[1], 4)) for row in (tf_df.select('filename', 'norm_keyword_freq').collect())], key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b24f7f-e267-4ad4-a937-4876f31cd2da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted([ (row[0], round(row[1], 4)) for row in (tf_df.select('filename', 'norm_keyword_freq', 'keyword_freq').collect()) if row[2] > 2], key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8331c47-e400-4df2-b55a-3e09d00351d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keyword TF cutoffs (absolute count and relative to article word count) used here\n",
    "# determined somewhat arbitrarily through visual inspection of sample articles.\n",
    "\n",
    "keyword_freq_cutoff = 3\n",
    "norm_keyword_freq_cutoff = 0.003\n",
    "\n",
    "filtered_articles_df = tf_df.filter((col('keyword_freq') >= keyword_freq_cutoff) & (col('norm_keyword_freq') >= norm_keyword_freq_cutoff))\n",
    "\n",
    "display(filtered_articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba28a94b-c7f0-48d2-b884-4f836eb98000",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "693fbaa3-4796-4359-913a-585102508575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb31f19-da2e-4b3c-a652-417b8634acd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "tokenize_sentences_udf = udf(tokenize_sentences, ArrayType(StringType()))\n",
    "\n",
    "sentence_tokenized_df = filtered_articles_df.withColumn('sentence', explode(tokenize_sentences_udf(filtered_articles_df['article_text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e09fd9e-42b2-485f-ad16-c777fb7993fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "sentence_score_udf = udf(sentiment_score, FloatType())\n",
    "\n",
    "sentence_sentiment_df = sentence_tokenized_df \\\n",
    "    .withColumn('sentiment_score', sentence_score_udf(sentence_tokenized_df['sentence'])) \\\n",
    "    .withColumn('keyword_in_sentence', col('sentence').contains(keyword))\n",
    "\n",
    "display(sentence_sentiment_df.select('filename', 'sentence', 'sentiment_score', 'keyword_in_sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5182f7eb-e94d-48c6-84de-fa25835b3cc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "613b4608-7b54-46ff-8c30-55db04af1d0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sentiment_summary_df_1 = sentence_sentiment_df.groupBy('filename').agg(F.mean('sentiment_score').alias('full_article_sentiment'))\n",
    "\n",
    "sentiment_summary_df_2 = sentence_sentiment_df.filter(col('keyword_in_sentence') == True).groupBy('filename').agg(F.mean('sentiment_score').alias('keyword_sentences_sentiment'))\n",
    "\n",
    "sentiment_summary_df = sentiment_summary_df_1.join(sentiment_summary_df_2, 'filename', 'inner') \\\n",
    "    .withColumn('full_article_sentiment', F.format_number('full_article_sentiment', 3)) \\\n",
    "    .withColumn('keyword_sentences_sentiment', F.format_number('keyword_sentences_sentiment', 3))\n",
    "\n",
    "display(sentiment_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a778055e-adb0-4baf-b822-4eebb079cd4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbeb51f6-b1eb-4cbd-b09f-a30dc3856752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unmount S3\n",
    "\n",
    "dbutils.fs.unmount(mount_location)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "News Sentiment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
